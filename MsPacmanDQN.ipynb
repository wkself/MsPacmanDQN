{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ6ar6kA+iffN/KgaXxsJm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install main Gymnasium library, Atari 2600 game familiy dependencies, and other Python libraries we'll be using for data manipulation & visualization"
      ],
      "metadata": {
        "id": "a0httjbA4OtH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy-nYgdC16s5"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari]\"\n",
        "!pip install \"gymnasium[accept-rom-license]\" #necessary for Atari envs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install seaborn\n",
        "!pip install tqdm\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "oXDipMgB5z2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modules for code"
      ],
      "metadata": {
        "id": "BKorWvIu6OY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict #avoid key errors\n",
        "\n",
        "import matplotlib.pyplot as plt #plots\n",
        "from matplotlib.patches import Patch #shapes\n",
        "import numpy as np\n",
        "import seaborn as sns #further data visualization\n",
        "from tqdm import tqdm #monitor progress for training\n",
        "\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "bxnhymqS6tom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create MsPacman environment, visualize the environment.\n",
        "Our seed resets the RNG of the environment and produces a random state upon resetting"
      ],
      "metadata": {
        "id": "xLBNbf8k8U2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from collections import defaultdict #avoid key errors\n",
        "\n",
        "#import matplotlib.pyplot as plt #plots\n",
        "#from matplotlib.patches import Patch #shapes\n",
        "#import numpy as np\n",
        "#import seaborn as sns #further data visualization\n",
        "#from tqdm import tqdm #monitor progress for training\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"ALE/MsPacman-v5\")\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "\n",
        "\n",
        "#we are sampling 1000 actions from a possible selection of 9\n",
        "#generally excluded actions have no effect in the specific game\n",
        "#this env is built with 9 actions but can be tuned to 18 to match Atari systems\n",
        "\n",
        "for _ in range(1000):\n",
        "  action = env.action_space.sample() #remember, sampling from 1 - 9 here\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  if terminated or truncated: #truncated, if the timer runs out\n",
        "    observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "i7lQw5ux8UUO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display #workaround to visualize env in jupyter notebook\n",
        "\n",
        "env = gym.make(\"ALE/MsPacman-v5\", render_mode='rgb_array')\n",
        "env.reset()\n",
        "\n",
        "plt.figure(figsize=(9, 9))\n",
        "plt.imshow(env.render())\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "env.close()\n",
        "\n",
        "#simple visualization of the game state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "3ghsdFlGu19u",
        "outputId": "89275991-4847-4322-b14f-3137b7f7f7e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x900 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, we've set up our environment and created a basic loop to visualize different actions. Let's create a DQN Agent to train our model. We'll be implementing the algorithm here."
      ],
      "metadata": {
        "id": "UNqDG3D9umHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First step is to import the libraries we'll use. PyTorch is the framework I'm using to build the model."
      ],
      "metadata": {
        "id": "taB8JGJGz2_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn #graphs\n",
        "import torch.optim as optim #further optimization algorithms\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "x-qsUYbbzkQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step is to define the DQN class. This contains the components of the algorithm for later implementation in the DQN agent."
      ],
      "metadata": {
        "id": "zj9NFt6Y0brX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, input_shape, num_actions):\n",
        "    super(DQN, self).__init__()\n",
        "    #applies linear transformation to the input data\n",
        "    #we define a neural network of 3 fully connected (fc) layers\n",
        "    #input shape is the shape of our input state\n",
        "    #num_actions is the variable representing total number of actions in env\n",
        "    #in our case, num_actions is set to discrete 9 per this Atari game env\n",
        "    self.fc1 = nn.Linear(input_shape, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, num_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Nkzt4PdC0mYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a replay buffer class, which will allow an agent to access their \"memories\" and train based on that data. The function below dictates the variables encapsulated in each experience pushed to the buffer. In this method those variables are self, state, action, reward, next_state, done."
      ],
      "metadata": {
        "id": "cHxM6s_E2oJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, capacity):\n",
        "    self.capacity = capacity #buffer has a fixed capacity\n",
        "    self.buffer = []\n",
        "    self.position = 0\n",
        "\n",
        "  def push(self, state, action, reward, next_state, done): #adds to buffer\n",
        "    if len(self.buffer) < self.capacity:\n",
        "      self.buffer.append(None) #overwrites the oldest experience if full\n",
        "    self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "    self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    #retrieves random batch of experiences from buffer\n",
        "    batch = random.sample(self.buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "    return np.array(states), actions, rewards, np.array(next_states), dones\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer) #returns number of experiences in buffer"
      ],
      "metadata": {
        "id": "nOWxbRfk2t7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the actual algorithms in our DQNAgent class. This is my first time implementing these algorithms in code and as such could be further optimized with different policies."
      ],
      "metadata": {
        "id": "b85hZynt5MP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent():\n",
        "  def __init__(self, env, replay_buffer, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay):\n",
        "    #our initialization method sets up our networks, optimizer, and loss func\n",
        "    self.env = env\n",
        "    self.replay_buffer = replay_buffer\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon_start\n",
        "    self.epsilon_end = epsilon_end\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "\n",
        "    self.policy_net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "    self.target_net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "    self.target_net.eval()\n",
        "\n",
        "    self.optimizer.optim.Adam(self.policy_net.parameters())\n",
        "    self.loss_fn = nn.SmoothL1Loss()\n",
        "\n",
        "  def select_action(self, state):\n",
        "    #selects action based on epsilon-greedy policy\n",
        "    if random.random() < self.epsilon:\n",
        "      return self.env.action_space.sample()\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        q_values = self.policy_net(torch.tensor(state, dtype=torch.float32))\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "  def train(self, num_episodes):\n",
        "    #runs training loop, we specify the number of episodes\n",
        "    #for every episode, agent interacts with environment, collects experiences,\n",
        "    #and updates the networks\n",
        "    episode_rewards = []\n",
        "    for episode in range(num_episodes):\n",
        "      state = self.env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "\n",
        "      while not done:\n",
        "        action = self.select_action(state)\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if len(self.replay_buffer) >= self.batch_size:\n",
        "          states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "          self.update_network(states, actions, rewards, next_states, dones)\n",
        "\n",
        "      self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "      episode_rewards.append(total_reward)\n",
        "\n",
        "      if episode % 100 == 0:\n",
        "        print(f\"Episode {episode}, Average Reward: {np.mean(episode_rewards[-100:])}\")\n",
        "\n",
        "  def update_network(self, states, actions, rewards, next_states, dones):\n",
        "    #calculates loss, performs gradient descent, updates policy network\n",
        "    #updates target network periodically\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "    dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "    q_values = self.policy_net(states).gather(1, actions)\n",
        "    next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
        "    expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = self.loss_fn(q_values, expected_q_values.unsqueeze(1))\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    self.update_target_network()\n",
        "\n",
        "  def update_target_network(self):\n",
        "    self.target_net.load_state_dict(self.policy_net.state_dict())"
      ],
      "metadata": {
        "id": "SpgP_Brj5bvQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}